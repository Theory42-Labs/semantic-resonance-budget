seed: 42
output_dir: experiments/phase4/reports
device: cuda

model:
  name: meta-llama/Meta-Llama-3-70B-Instruct
  max_new_tokens: 192
  temperature: 0.6
  top_p: 0.95

experiment:
  runs: 120                 # you can CLI-override (e.g., 150â€“200 later)
  buckets: ["creative", "factual", "reasoning"]
  # Ensure 'baseline' appears first so we can compute ref_similarity per prompt/run
  interventions: ["baseline", "embed_noise", "delay_think", "force_chain", "force_chain_strict", "self_anchor", "self_reflective_guard"]
  verifier_model: "sentence-transformers/all-MiniLM-L6-v2"

metrics:
  transfer_entropy:
    history_window: 8
    jitter: 1e-6
    estimator: ksg

  cross_entropy:
    baseline_model: "microsoft/Phi-2"   # fast, repeat later with Phi-2 for robustness
    device: cpu

  verifier:
    device: cpu

  semantic_primitives:
    nsm_list_path: experiments/phase3/vendors/nsm_primitives.txt  # reuse for now (or copy to phase4/vendors)
    weight: 1.0

  geometry:
    collect_token_embeddings: true
    umap:
      n_neighbors: 20
      min_dist: 0.15
      n_components: 2
      metric: euclidean
      random_state: 42
    tda:
      compute_persistent_homology: true
      max_dim: 1

logging:
  csv: true
  jsonl: true
  plots: true     # set true when you want auto-plots from the runner