{
  "config": {
    "seed": 42,
    "output_dir": "experiments/phase4/reports",
    "device": "mps",
    "model_name": "meta-llama/Llama-3.2-3B-Instruct",
    "max_new_tokens": 160,
    "temperature": 0.6,
    "top_p": 0.95,
    "runs": 120,
    "buckets": [
      "creative",
      "factual",
      "reasoning"
    ],
    "interventions": [
      "baseline",
      "embed_noise",
      "delay_think",
      "force_chain",
      "force_chain_strict",
      "self_anchor",
      "self_reflective_guard"
    ],
    "verifier_model": "sentence-transformers/all-MiniLM-L6-v2",
    "metrics": {
      "transfer_entropy": {
        "history_window": 3,
        "estimator": "ksg"
      },
      "cross_entropy": {
        "baseline_model": "microsoft/Phi-2"
      },
      "semantic_primitives": {
        "nsm_list_path": "experiments/phase3/vendors/nsm_primitives.txt",
        "weight": 1.0
      },
      "geometry": {
        "collect_token_embeddings": true,
        "umap": {
          "n_neighbors": 10,
          "min_dist": 0.15,
          "n_components": 2,
          "metric": "euclidean"
        },
        "tda": {
          "compute_persistent_homology": true,
          "max_dim": 1
        }
      }
    }
  },
  "capabilities": {
    "umap_used": true,
    "ripser_used": true,
    "hf_nll_used": true,
    "hf_verifier_used": true,
    "model_name": "meta-llama/Llama-3.2-3B-Instruct",
    "verifier_model": "sentence-transformers/all-MiniLM-L6-v2",
    "device": "mps",
    "temperature": 0.6
  },
  "timestamp": "2025-11-01T12:21:22.105865"
}